"""
Exploitability computation for poker solver evaluation.

This module provides empirical exploitability estimation via Monte Carlo sampling.
Unlike exact best-response computation (which is infeasible for large poker games),
this uses rollout-based approximation as described in modern poker research.

IMPORTANT: These are *estimates* with confidence intervals, not exact exploitability.
The estimation quality depends on num_samples and rollout_depth.

Target exploitability values (in milli-big-blinds per game):
- 100+ mbb/g: Very exploitable (random play)
- 20-100 mbb/g: Weak player
- 5-20 mbb/g: Decent player
- 1-5 mbb/g: Good player
- 0.1-1 mbb/g: Strong player
- < 0.1 mbb/g: Near-optimal (professional level)

References:
- Johanson et al. "Evaluating State-Space Abstractions in Extensive-Form Games" (2013)
- Brown & Sandholm "Solving Imperfect-Information Games via Discounted Regret Minimization" (2019)
"""

import numpy as np

from src.engine.solver.infoset_encoder import encode_infoset_key
from src.engine.solver.mccfr import MCCFRSolver


def compute_exploitability(
    solver: MCCFRSolver,
    num_samples: int = 10000,
    use_average_strategy: bool = True,
    num_rollouts_per_infoset: int = 100,
    seed: int | None = None,
) -> dict[str, float | int | tuple[float, float]]:
    """
    Compute empirical exploitability estimate via Monte Carlo rollout sampling.

    IMPORTANT: This is NOT exact exploitability. This estimates exploitability by:
    1. Freezing the solver's strategy σ
    2. For each player, simulating games where:
       - BR player uses greedy rollout policy (best immediate action + sampling)
       - Opponent samples actions from frozen strategy σ
    3. Estimating expected value gain

    This approximation is used in modern poker research when exact best-response
    computation is infeasible due to game tree size.

    Exploitability = (BR_utility_p0 + BR_utility_p1) / 2

    Args:
        solver: Trained MCCFR solver with frozen strategy
        num_samples: Number of game simulations per player
        use_average_strategy: If True, use average strategy (correct for Nash approximation)
                            If False, use current strategy
        num_rollouts_per_infoset: Number of random rollouts to estimate action values
                                 Higher = more accurate BR, but slower
        seed: Random seed for reproducibility

    Returns:
        Dictionary with exploitability metrics:
        {
            'exploitability_mbb': float (milli-big-blinds per game),
            'exploitability_bb': float (big-blinds per game),
            'player_0_br_utility': float (raw chip value),
            'player_1_br_utility': float (raw chip value),
            'std_error_mbb': float (standard error of estimate),
            'confidence_95_mbb': tuple (lower, upper bounds),
            'num_samples': int,
        }

    Notes:
        - This is an *empirical estimate* with statistical uncertainty
        - Increase num_samples to reduce standard error
        - Increase num_rollouts_per_infoset for better BR approximation
        - Confidence intervals use paired samples per initial state
          (no independence assumption between player BR utilities)
        - Results are most reliable for well-converged strategies
    """
    if num_samples <= 0:
        return {
            "exploitability_mbb": float("nan"),
            "exploitability_bb": float("nan"),
            "player_0_br_utility": float("nan"),
            "player_1_br_utility": float("nan"),
            "std_error_mbb": float("nan"),
            "confidence_95_mbb": (float("nan"), float("nan")),
            "num_samples": int(num_samples),
        }

    if seed is not None:
        np.random.seed(seed)

    # Store per-player BR utilities and paired exploitability samples.
    # For each initial state i, we compute:
    #   e_i = (u_i_player0 + u_i_player1) / 2
    # Standard error must be computed from {e_i} directly because
    # u_i_player0 and u_i_player1 are sampled from the same state and can be correlated.
    br_utilities: list[list[float]] = [[], []]
    exploitability_samples: list[float] = []

    for _ in range(num_samples):
        # Sample a random hand
        state = solver._deal_initial_state()

        # Compute best response utility for each player
        utility_p0 = _compute_rollout_best_response(
            solver, state, 0, use_average_strategy, num_rollouts_per_infoset
        )
        utility_p1 = _compute_rollout_best_response(
            solver, state, 1, use_average_strategy, num_rollouts_per_infoset
        )
        br_utilities[0].append(utility_p0)
        br_utilities[1].append(utility_p1)
        exploitability_samples.append((utility_p0 + utility_p1) / 2)

    # Compute statistics
    br_mean = [np.mean(br_utilities[0]), np.mean(br_utilities[1])]
    exploitability = np.mean(exploitability_samples)
    if num_samples < 2:
        exploitability_se = 0.0
    else:
        # Paired-sample SE (does not assume independence between player BR utilities).
        exploitability_se = np.std(exploitability_samples, ddof=1) / np.sqrt(num_samples)

    # Convert to milli-big-blinds per game (mbb/g)
    big_blind = solver.config.game.big_blind
    exploitability_mbb = (exploitability / big_blind) * 1000
    se_mbb = (exploitability_se / big_blind) * 1000

    # 95% confidence interval (±1.96 SE)
    ci_lower = exploitability_mbb - 1.96 * se_mbb
    ci_upper = exploitability_mbb + 1.96 * se_mbb

    return {
        "exploitability_mbb": float(exploitability_mbb),
        "exploitability_bb": float(exploitability / big_blind),
        "player_0_br_utility": float(br_mean[0]),
        "player_1_br_utility": float(br_mean[1]),
        "std_error_mbb": float(se_mbb),
        "confidence_95_mbb": (float(ci_lower), float(ci_upper)),
        "num_samples": int(num_samples),
    }


def _compute_rollout_best_response(
    solver: MCCFRSolver,
    state,
    br_player: int,
    use_average_strategy: bool,
    num_rollouts: int,
) -> float:
    """
    Compute approximate best response utility via rollout simulation.

    This is a practical approximation used in modern poker research. Instead of
    exact tree traversal (infeasible), we:
    1. At BR player's decision points: estimate action values via Monte Carlo rollouts
    2. Choose greedily based on rollout estimates
    3. At opponent's decision points: sample from their frozen strategy

    This avoids the fatal flaws of naive recursion:
    - No chance node sampling (breaks BR guarantees)
    - No full tree expansion (computationally infeasible)
    - Uses simulation-based action evaluation

    Args:
        solver: MCCFR solver with trained strategy
        state: Current game state
        br_player: Player computing best response (0 or 1)
        use_average_strategy: Whether to use average strategy or current strategy
        num_rollouts: Number of rollouts to estimate each action's value

    Returns:
        Expected utility for br_player in this simulation
    """
    # Terminal node - return payoff
    if state.is_terminal:
        # If terminal but board incomplete (all-in before river), deal remaining cards
        if len(state.board) < 5:
            complete_state = solver._deal_remaining_cards(state)
            return complete_state.get_payoff(br_player, solver.rules)
        return state.get_payoff(br_player, solver.rules)

    # Chance node - deal next card and continue
    if solver._is_chance_node(state):
        next_state = solver._sample_chance_outcome(state)
        return _compute_rollout_best_response(
            solver, next_state, br_player, use_average_strategy, num_rollouts
        )

    # Decision node
    current_player = state.current_player

    # Get legal actions
    legal_actions = solver.action_model.get_legal_actions(state)

    if not legal_actions:
        raise ValueError(f"No legal actions at state: {state}")

    if current_player == br_player:
        # BR player: estimate action values via rollouts, choose best
        action_values = []

        for action in legal_actions:
            # Estimate this action's value via multiple rollouts
            total_value = 0.0
            for _ in range(num_rollouts):
                next_state = state.apply_action(action, solver.rules)
                # Continue rollout using same policy
                value = _rollout_from_state(
                    solver, next_state, br_player, use_average_strategy, num_rollouts
                )
                total_value += value

            avg_value = total_value / num_rollouts
            action_values.append(avg_value)

        # Return value of best action
        return max(action_values)

    else:
        # Opponent: sample from their strategy
        infoset_key = encode_infoset_key(state, current_player, solver.card_abstraction)
        infoset = solver.storage.get_infoset(infoset_key)
        legal_actions, strategy = _select_strategy(infoset, legal_actions, use_average_strategy)

        # Sample action from opponent's strategy
        action_idx = np.random.choice(len(legal_actions), p=strategy)
        chosen_action = legal_actions[action_idx]
        next_state = state.apply_action(chosen_action, solver.rules)

        # Continue rollout
        return _compute_rollout_best_response(
            solver, next_state, br_player, use_average_strategy, num_rollouts
        )


def _rollout_from_state(
    solver: MCCFRSolver,
    state,
    br_player: int,
    use_average_strategy: bool,
    num_rollouts: int,
) -> float:
    """
    Complete a single rollout from given state to terminal.

    Both players sample from their policies (BR player also samples, not greedy here).
    This is used within the rollout-based action value estimation.

    Args:
        solver: MCCFR solver
        state: Current state
        br_player: Player whose utility we're computing
        use_average_strategy: Strategy type to use
        num_rollouts: Rollout budget (passed through for consistency)

    Returns:
        Terminal utility for br_player
    """
    # Terminal node
    if state.is_terminal:
        if len(state.board) < 5:
            complete_state = solver._deal_remaining_cards(state)
            return complete_state.get_payoff(br_player, solver.rules)
        return state.get_payoff(br_player, solver.rules)

    # Chance node
    if solver._is_chance_node(state):
        next_state = solver._sample_chance_outcome(state)
        return _rollout_from_state(
            solver, next_state, br_player, use_average_strategy, num_rollouts
        )

    # Decision node - sample from policy
    current_player = state.current_player
    legal_actions = solver.action_model.get_legal_actions(state)

    if not legal_actions:
        raise ValueError(f"No legal actions at state: {state}")

    # Get strategy for current player
    infoset_key = encode_infoset_key(state, current_player, solver.card_abstraction)
    infoset = solver.storage.get_infoset(infoset_key)

    legal_actions, strategy = _select_strategy(infoset, legal_actions, use_average_strategy)

    # Sample action and continue
    action_idx = np.random.choice(len(legal_actions), p=strategy)
    chosen_action = legal_actions[action_idx]
    next_state = state.apply_action(chosen_action, solver.rules)

    return _rollout_from_state(solver, next_state, br_player, use_average_strategy, num_rollouts)


def _select_strategy(
    infoset,
    legal_actions: list,
    use_average_strategy: bool,
) -> tuple[list, np.ndarray]:
    if infoset is None:
        return legal_actions, np.ones(len(legal_actions)) / len(legal_actions)

    legal_set = set(legal_actions)
    valid_indices = [i for i, action in enumerate(infoset.legal_actions) if action in legal_set]
    if not valid_indices:
        return legal_actions, np.ones(len(legal_actions)) / len(legal_actions)

    filtered_actions = [infoset.legal_actions[i] for i in valid_indices]
    strategy = infoset.get_filtered_strategy(
        valid_indices=valid_indices, use_average=use_average_strategy
    )
    return filtered_actions, strategy


def compute_total_positive_regret(solver: MCCFRSolver) -> dict[str, float]:
    """
    Compute total positive regret across all information sets.

    This is a training diagnostic metric that measures cumulative regret.
    Unlike exploitability, this is NOT comparable across different abstractions
    and is NOT interpretable in big-blind terms.

    Use this to:
    - Monitor training convergence (should decrease over iterations)
    - Compare different training runs with SAME abstraction
    - Debug regret accumulation issues

    Do NOT use this to:
    - Compare solution quality across different abstractions
    - Report final solution quality (use exploitability instead)
    - Make claims about skill level

    Note: Often called "NashConv" in research, but that name is misleading
    because it's not directly related to Nash equilibrium distance.

    Args:
        solver: Trained solver

    Returns:
        Dictionary with regret metrics:
        {
            'total_positive_regret': float,
            'num_infosets': int,
            'avg_regret_per_infoset': float,
        }
    """
    total_regret = 0.0
    num_infosets = 0

    infosets = solver.storage.iter_infosets()

    # Iterate over all infosets
    for infoset in infosets:
        # Sum positive regrets (CFR+ floors at 0, but we compute explicitly)
        positive_regrets = np.maximum(infoset.regrets, 0)
        total_regret += np.sum(positive_regrets)
        num_infosets += 1

    avg_regret = total_regret / num_infosets if num_infosets > 0 else 0.0

    return {
        "total_positive_regret": total_regret,
        "num_infosets": num_infosets,
        "avg_regret_per_infoset": avg_regret,
    }
