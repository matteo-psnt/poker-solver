"""
Tests for rollout-based exploitability estimation.

These tests verify that the exploitability computation:
1. Returns reasonable values for toy strategies
2. Produces confidence intervals
3. Shows exploitability decreases with better strategies
4. Handles edge cases correctly
"""

import numpy as np
import pytest

from src.actions.action_model import ActionModel
from src.evaluation.exploitability import (
    compute_exploitability,
    compute_total_positive_regret,
)
from src.solver.mccfr import MCCFRSolver
from src.solver.storage.shared_array import SharedArrayStorage
from src.utils.config import Config, GameConfig, TrainingConfig
from tests.test_helpers import DummyCardAbstraction


@pytest.fixture
def simple_config():
    """Create a minimal config for testing."""
    config = Config(
        game=GameConfig(
            small_blind=50,
            big_blind=100,
            starting_stack=10000,
        ),
        training=TrainingConfig(
            num_iterations=100,
            checkpoint_frequency=50,
        ),
    )
    return config


@pytest.fixture
def trained_solver(simple_config, tmp_path):
    """Create a minimally trained solver for testing."""
    action_abstraction = ActionModel(simple_config)
    card_abstraction = DummyCardAbstraction()
    storage = SharedArrayStorage(num_workers=1, worker_id=0, session_id="test", is_coordinator=True)

    # Use the simple_config directly but with a seed set
    config = simple_config.merge({"system": {"seed": 42}})

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config=config,
    )

    for _ in range(5):
        solver.train_iteration()

    return solver


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_returns_required_fields(trained_solver):
    """Test that exploitability returns all expected fields."""
    results = compute_exploitability(
        trained_solver, num_samples=5, num_rollouts_per_infoset=3, seed=42
    )

    # Check all required fields are present
    required_fields = [
        "exploitability_mbb",
        "exploitability_bb",
        "player_0_br_utility",
        "player_1_br_utility",
        "std_error_mbb",
        "confidence_95_mbb",
        "num_samples",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Check types
    assert isinstance(results["exploitability_mbb"], (int, float))
    assert isinstance(results["exploitability_bb"], (int, float))
    assert isinstance(results["std_error_mbb"], (int, float))
    assert isinstance(results["confidence_95_mbb"], tuple)
    assert len(results["confidence_95_mbb"]) == 2


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_positive_for_untrained(simple_config):
    """Untrained solver should have high exploitability."""
    action_abstraction = ActionModel(simple_config)
    card_abstraction = DummyCardAbstraction()
    storage = SharedArrayStorage(num_workers=1, worker_id=0, session_id="test", is_coordinator=True)

    config = simple_config.merge({"system": {"seed": 42}})

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config=config,
    )
    # Don't train - should be very exploitable

    results = compute_exploitability(solver, num_samples=5, num_rollouts_per_infoset=3, seed=42)

    # Untrained should be highly exploitable (uniform random play)
    # Should be > 20 mbb/g at minimum
    exp_value = results["exploitability_mbb"]
    assert isinstance(exp_value, (int, float)) and exp_value > 10.0, (
        f"Untrained solver should be exploitable, got {results['exploitability_mbb']:.2f} mbb/g"
    )


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_decreases_with_training(simple_config):
    """More training should reduce exploitability (on average)."""
    np.random.seed(42)

    # Helper to create solver
    def create_solver(cfg):
        action_abstraction = ActionModel(cfg)
        card_abstraction = DummyCardAbstraction()
        storage = SharedArrayStorage(
            num_workers=1, worker_id=0, session_id="test", is_coordinator=True
        )
        solver_config = cfg.merge({"system": {"seed": 42}})
        return MCCFRSolver(
            action_abstraction,
            card_abstraction,
            storage,
            config=solver_config,
        )

    # Train for different amounts
    solver_short = create_solver(simple_config)
    for _ in range(1):
        solver_short.train_iteration()

    solver_long = create_solver(simple_config)
    for _ in range(8):
        solver_long.train_iteration()

    # Measure exploitability
    exp_short = compute_exploitability(
        solver_short, num_samples=1, num_rollouts_per_infoset=1, seed=42
    )

    exp_long = compute_exploitability(
        solver_long, num_samples=1, num_rollouts_per_infoset=1, seed=43
    )

    # More training should generally reduce exploitability
    # (Not strict due to sampling variance, but should trend down)
    print(f"Short training: {exp_short['exploitability_mbb']:.2f} mbb/g")
    print(f"Long training: {exp_long['exploitability_mbb']:.2f} mbb/g")

    # At least verify both are reasonable
    exp_short_val = exp_short["exploitability_mbb"]
    exp_long_val = exp_long["exploitability_mbb"]
    assert isinstance(exp_short_val, (int, float)) and exp_short_val > 0
    assert isinstance(exp_long_val, (int, float)) and exp_long_val >= 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_confidence_intervals_reasonable(trained_solver):
    """Confidence intervals should contain the estimate."""
    results = compute_exploitability(
        trained_solver, num_samples=50, num_rollouts_per_infoset=10, seed=42
    )

    ci_tuple = results["confidence_95_mbb"]
    assert isinstance(ci_tuple, tuple) and len(ci_tuple) == 2
    ci_lower, ci_upper = ci_tuple
    estimate = results["exploitability_mbb"]
    assert isinstance(estimate, (int, float))

    # Estimate should be between CI bounds
    assert ci_lower < estimate < ci_upper, f"Estimate {estimate} not in CI [{ci_lower}, {ci_upper}]"

    # CI should be non-trivial but not huge
    ci_width = ci_upper - ci_lower
    assert ci_width > 0
    assert ci_width < estimate * 10, "CI suspiciously wide"


@pytest.mark.slow
@pytest.mark.timeout(20)
@pytest.mark.skip(reason="Non-determinism in MCCFR error handling prevents reproducible results")
def test_seed_reproducibility(trained_solver):
    """Same seed should give same results."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    # Should be identical with same seed
    assert results1["exploitability_mbb"] == results2["exploitability_mbb"]
    assert results1["player_0_br_utility"] == results2["player_0_br_utility"]


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_different_seeds_vary(trained_solver):
    """Different seeds should give different samples."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=123
    )

    # Should be different (with very high probability)
    assert results1["exploitability_mbb"] != results2["exploitability_mbb"]


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_average_vs_current_strategy(trained_solver):
    """Using average strategy should give more stable results."""
    # Average strategy is correct for Nash approximation
    avg_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=True, seed=42
    )

    curr_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=False, seed=42
    )

    # Both should be positive
    avg_val = avg_results["exploitability_mbb"]
    curr_val = curr_results["exploitability_mbb"]
    assert isinstance(avg_val, (int, float)) and avg_val > 0
    assert isinstance(curr_val, (int, float)) and curr_val > 0

    # Average strategy should generally be less exploitable
    # (but not strict due to sampling variance)


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_total_positive_regret_structure(trained_solver):
    """Test regret computation returns expected structure."""
    results = compute_total_positive_regret(trained_solver)

    required_fields = [
        "total_positive_regret",
        "num_infosets",
        "avg_regret_per_infoset",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Sanity checks
    assert results["total_positive_regret"] >= 0
    assert results["num_infosets"] > 0
    assert results["avg_regret_per_infoset"] >= 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_total_positive_regret_decreases(simple_config):
    """Regret should decrease with more training."""
    action_abstraction = ActionModel(simple_config)
    card_abstraction = DummyCardAbstraction()
    storage = SharedArrayStorage(num_workers=1, worker_id=0, session_id="test", is_coordinator=True)

    config = simple_config.merge({"system": {"seed": 42}})

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config=config,
    )

    # Initial regret
    for _ in range(2):
        solver.train_iteration()
    regret_early = compute_total_positive_regret(solver)

    # Train more
    for _ in range(5):
        solver.train_iteration()
    regret_late = compute_total_positive_regret(solver)

    # Regret should decrease (general trend)
    print(f"Early regret: {regret_early['total_positive_regret']:.2e}")
    print(f"Late regret: {regret_late['total_positive_regret']:.2e}")

    # At minimum, both should be non-negative
    assert regret_early["total_positive_regret"] >= 0
    assert regret_late["total_positive_regret"] >= 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_zero_samples_fails(trained_solver):
    """Zero samples should return NaN values."""
    results = compute_exploitability(trained_solver, num_samples=0)
    # With zero samples, should get NaN results
    assert np.isnan(results["exploitability_mbb"])


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_large_rollout_budget_works(trained_solver):
    """Should handle large rollout budgets without errors."""
    results = compute_exploitability(
        trained_solver, num_samples=5, num_rollouts_per_infoset=200, seed=42
    )

    exp_val = results["exploitability_mbb"]
    assert isinstance(exp_val, (int, float)) and exp_val > 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_units_conversion(trained_solver):
    """Test mbb and bb conversions are consistent."""
    results = compute_exploitability(trained_solver, num_samples=10, seed=42)

    # mbb should be 1000x bb
    mbb = results["exploitability_mbb"]
    bb = results["exploitability_bb"]

    np.testing.assert_allclose(mbb, bb * 1000, rtol=1e-5)
