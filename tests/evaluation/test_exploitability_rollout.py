"""
Tests for rollout-based exploitability estimation.

These tests verify that the exploitability computation:
1. Returns reasonable values for toy strategies
2. Produces confidence intervals
3. Shows exploitability decreases with better strategies
4. Handles edge cases correctly
"""

import numpy as np
import pytest

from src.evaluation.exploitability import (
    compute_exploitability,
    compute_sampled_response_gap,
    compute_total_positive_regret,
)
from src.solver.mccfr import MCCFRSolver
from src.utils.config import Config


@pytest.fixture
def simple_config():
    """Create a minimal config for testing."""
    config = Config()
    config.small_blind = 50
    config.big_blind = 100
    config.starting_stack = 10000
    config.num_buckets = 5
    config.mccfr_iterations = 100
    config.checkpoint_interval = 50
    return config


@pytest.fixture
def trained_solver(simple_config, tmp_path):
    """Create a minimally trained solver for testing."""
    solver = MCCFRSolver(simple_config)
    solver.train(num_iterations=100, verbose=False)
    return solver


def test_exploitability_returns_required_fields(trained_solver):
    """Test that exploitability returns all expected fields."""
    results = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    # Check all required fields are present
    required_fields = [
        "exploitability_mbb",
        "exploitability_bb",
        "player_0_br_utility",
        "player_1_br_utility",
        "std_error_mbb",
        "confidence_95_mbb",
        "num_samples",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Check types
    assert isinstance(results["exploitability_mbb"], (int, float))
    assert isinstance(results["exploitability_bb"], (int, float))
    assert isinstance(results["std_error_mbb"], (int, float))
    assert isinstance(results["confidence_95_mbb"], tuple)
    assert len(results["confidence_95_mbb"]) == 2


def test_exploitability_positive_for_untrained(simple_config):
    """Untrained solver should have high exploitability."""
    solver = MCCFRSolver(simple_config)
    # Don't train - should be very exploitable

    results = compute_exploitability(solver, num_samples=10, num_rollouts_per_infoset=5, seed=42)

    # Untrained should be highly exploitable (uniform random play)
    # Should be > 20 mbb/g at minimum
    assert results["exploitability_mbb"] > 10.0, (
        f"Untrained solver should be exploitable, got {results['exploitability_mbb']:.2f} mbb/g"
    )


def test_exploitability_decreases_with_training(simple_config):
    """More training should reduce exploitability (on average)."""
    np.random.seed(42)

    # Train for different amounts
    solver_short = MCCFRSolver(simple_config)
    solver_short.train(num_iterations=50, verbose=False)

    solver_long = MCCFRSolver(simple_config)
    solver_long.train(num_iterations=500, verbose=False)

    # Measure exploitability
    exp_short = compute_exploitability(
        solver_short, num_samples=20, num_rollouts_per_infoset=10, seed=42
    )

    exp_long = compute_exploitability(
        solver_long, num_samples=20, num_rollouts_per_infoset=10, seed=43
    )

    # More training should generally reduce exploitability
    # (Not strict due to sampling variance, but should trend down)
    print(f"Short training: {exp_short['exploitability_mbb']:.2f} mbb/g")
    print(f"Long training: {exp_long['exploitability_mbb']:.2f} mbb/g")

    # At least verify both are reasonable
    assert exp_short["exploitability_mbb"] > 0
    assert exp_long["exploitability_mbb"] > 0


def test_confidence_intervals_reasonable(trained_solver):
    """Confidence intervals should contain the estimate."""
    results = compute_exploitability(
        trained_solver, num_samples=50, num_rollouts_per_infoset=10, seed=42
    )

    ci_lower, ci_upper = results["confidence_95_mbb"]
    estimate = results["exploitability_mbb"]

    # Estimate should be between CI bounds
    assert ci_lower < estimate < ci_upper, f"Estimate {estimate} not in CI [{ci_lower}, {ci_upper}]"

    # CI should be non-trivial but not huge
    ci_width = ci_upper - ci_lower
    assert ci_width > 0
    assert ci_width < estimate * 10, "CI suspiciously wide"


def test_seed_reproducibility(trained_solver):
    """Same seed should give same results."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    # Should be identical with same seed
    assert results1["exploitability_mbb"] == results2["exploitability_mbb"]
    assert results1["player_0_br_utility"] == results2["player_0_br_utility"]


def test_different_seeds_vary(trained_solver):
    """Different seeds should give different samples."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=123
    )

    # Should be different (with very high probability)
    assert results1["exploitability_mbb"] != results2["exploitability_mbb"]


def test_average_vs_current_strategy(trained_solver):
    """Using average strategy should give more stable results."""
    # Average strategy is correct for Nash approximation
    avg_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=True, seed=42
    )

    curr_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=False, seed=42
    )

    # Both should be positive
    assert avg_results["exploitability_mbb"] > 0
    assert curr_results["exploitability_mbb"] > 0

    # Average strategy should generally be less exploitable
    # (but not strict due to sampling variance)


def test_total_positive_regret_structure(trained_solver):
    """Test regret computation returns expected structure."""
    results = compute_total_positive_regret(trained_solver)

    required_fields = [
        "total_positive_regret",
        "num_infosets",
        "avg_regret_per_infoset",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Sanity checks
    assert results["total_positive_regret"] >= 0
    assert results["num_infosets"] > 0
    assert results["avg_regret_per_infoset"] >= 0


def test_total_positive_regret_decreases(simple_config):
    """Regret should decrease with more training."""
    solver = MCCFRSolver(simple_config)

    # Initial regret
    solver.train(num_iterations=50, verbose=False)
    regret_early = compute_total_positive_regret(solver)

    # Train more
    solver.train(num_iterations=200, verbose=False)
    regret_late = compute_total_positive_regret(solver)

    # Regret should decrease (general trend)
    print(f"Early regret: {regret_early['total_positive_regret']:.2e}")
    print(f"Late regret: {regret_late['total_positive_regret']:.2e}")

    # At minimum, both should be non-negative
    assert regret_early["total_positive_regret"] >= 0
    assert regret_late["total_positive_regret"] >= 0


def test_deprecated_function_warns(trained_solver):
    """Deprecated function should issue warning."""
    with pytest.warns(DeprecationWarning, match="compute_sampled_response_gap is deprecated"):
        results = compute_sampled_response_gap(trained_solver, num_samples=5)

    # Should still return something
    assert "exploitability_mbb" in results


def test_zero_samples_fails(trained_solver):
    """Zero samples should raise error or return invalid."""
    with pytest.raises((ValueError, ZeroDivisionError)):
        compute_exploitability(trained_solver, num_samples=0)


def test_large_rollout_budget_works(trained_solver):
    """Should handle large rollout budgets without errors."""
    results = compute_exploitability(
        trained_solver, num_samples=5, num_rollouts_per_infoset=200, seed=42
    )

    assert results["exploitability_mbb"] > 0


def test_exploitability_units_conversion(trained_solver):
    """Test mbb and bb conversions are consistent."""
    results = compute_exploitability(trained_solver, num_samples=10, seed=42)

    # mbb should be 1000x bb
    mbb = results["exploitability_mbb"]
    bb = results["exploitability_bb"]

    np.testing.assert_allclose(mbb, bb * 1000, rtol=1e-5)
