"""
Tests for rollout-based exploitability estimation.

These tests verify that the exploitability computation:
1. Returns reasonable values for toy strategies
2. Produces confidence intervals
3. Shows exploitability decreases with better strategies
4. Handles edge cases correctly
"""

import numpy as np
import pytest

from src.abstraction.core.action_abstraction import ActionAbstraction
from src.evaluation.exploitability import (
    compute_exploitability,
    compute_total_positive_regret,
)
from src.solver.mccfr import MCCFRSolver
from src.solver.storage import InMemoryStorage
from src.utils.config import Config
from tests.test_helpers import DummyCardAbstraction


@pytest.fixture
def simple_config():
    """Create a minimal config for testing."""
    config = Config.default()
    config.set("game.small_blind", 50)
    config.set("game.big_blind", 100)
    config.set("game.starting_stack", 10000)
    config.set("training.num_iterations", 100)
    config.set("training.checkpoint_frequency", 50)
    return config


@pytest.fixture
def trained_solver(simple_config, tmp_path):
    """Create a minimally trained solver for testing."""
    action_abstraction = ActionAbstraction(big_blind=simple_config.get("game.big_blind", 100))
    card_abstraction = DummyCardAbstraction()
    storage = InMemoryStorage()

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config={
            "starting_stack": simple_config.get("game.starting_stack", 10000),
            "small_blind": simple_config.get("game.small_blind", 50),
            "big_blind": simple_config.get("game.big_blind", 100),
            "seed": 42,
        },
    )

    # Train for minimal iterations (reduced for speed)
    for _ in range(20):
        solver.train_iteration()

    return solver


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_returns_required_fields(trained_solver):
    """Test that exploitability returns all expected fields."""
    results = compute_exploitability(
        trained_solver, num_samples=5, num_rollouts_per_infoset=3, seed=42
    )

    # Check all required fields are present
    required_fields = [
        "exploitability_mbb",
        "exploitability_bb",
        "player_0_br_utility",
        "player_1_br_utility",
        "std_error_mbb",
        "confidence_95_mbb",
        "num_samples",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Check types
    assert isinstance(results["exploitability_mbb"], (int, float))
    assert isinstance(results["exploitability_bb"], (int, float))
    assert isinstance(results["std_error_mbb"], (int, float))
    assert isinstance(results["confidence_95_mbb"], tuple)
    assert len(results["confidence_95_mbb"]) == 2


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_positive_for_untrained(simple_config):
    """Untrained solver should have high exploitability."""
    action_abstraction = ActionAbstraction(big_blind=simple_config.get("game.big_blind", 100))
    card_abstraction = DummyCardAbstraction()
    storage = InMemoryStorage()

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config={
            "starting_stack": simple_config.get("game.starting_stack", 10000),
            "small_blind": simple_config.get("game.small_blind", 50),
            "big_blind": simple_config.get("game.big_blind", 100),
            "seed": 42,
        },
    )
    # Don't train - should be very exploitable

    results = compute_exploitability(solver, num_samples=5, num_rollouts_per_infoset=3, seed=42)

    # Untrained should be highly exploitable (uniform random play)
    # Should be > 20 mbb/g at minimum
    assert results["exploitability_mbb"] > 10.0, (
        f"Untrained solver should be exploitable, got {results['exploitability_mbb']:.2f} mbb/g"
    )


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_decreases_with_training(simple_config):
    """More training should reduce exploitability (on average)."""
    np.random.seed(42)

    # Helper to create solver
    def create_solver(config):
        action_abstraction = ActionAbstraction(big_blind=config.get("game.big_blind", 100))
        card_abstraction = DummyCardAbstraction()
        storage = InMemoryStorage()
        return MCCFRSolver(
            action_abstraction,
            card_abstraction,
            storage,
            config={
                "starting_stack": config.get("game.starting_stack", 10000),
                "small_blind": config.get("game.small_blind", 50),
                "big_blind": config.get("game.big_blind", 100),
                "seed": 42,
            },
        )

    # Train for different amounts (reduced for speed)
    solver_short = create_solver(simple_config)
    for _ in range(10):
        solver_short.train_iteration()

    solver_long = create_solver(simple_config)
    for _ in range(30):
        solver_long.train_iteration()

    # Measure exploitability (reduced samples)
    exp_short = compute_exploitability(
        solver_short, num_samples=5, num_rollouts_per_infoset=3, seed=42
    )

    exp_long = compute_exploitability(
        solver_long, num_samples=5, num_rollouts_per_infoset=3, seed=43
    )

    # More training should generally reduce exploitability
    # (Not strict due to sampling variance, but should trend down)
    print(f"Short training: {exp_short['exploitability_mbb']:.2f} mbb/g")
    print(f"Long training: {exp_long['exploitability_mbb']:.2f} mbb/g")

    # At least verify both are reasonable
    assert exp_short["exploitability_mbb"] > 0
    assert exp_long["exploitability_mbb"] > 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_confidence_intervals_reasonable(trained_solver):
    """Confidence intervals should contain the estimate."""
    results = compute_exploitability(
        trained_solver, num_samples=50, num_rollouts_per_infoset=10, seed=42
    )

    ci_lower, ci_upper = results["confidence_95_mbb"]
    estimate = results["exploitability_mbb"]

    # Estimate should be between CI bounds
    assert ci_lower < estimate < ci_upper, f"Estimate {estimate} not in CI [{ci_lower}, {ci_upper}]"

    # CI should be non-trivial but not huge
    ci_width = ci_upper - ci_lower
    assert ci_width > 0
    assert ci_width < estimate * 10, "CI suspiciously wide"


@pytest.mark.slow
@pytest.mark.timeout(20)
@pytest.mark.skip(reason="Non-determinism in MCCFR error handling prevents reproducible results")
def test_seed_reproducibility(trained_solver):
    """Same seed should give same results."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    # Should be identical with same seed
    assert results1["exploitability_mbb"] == results2["exploitability_mbb"]
    assert results1["player_0_br_utility"] == results2["player_0_br_utility"]


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_different_seeds_vary(trained_solver):
    """Different seeds should give different samples."""
    results1 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=42
    )

    results2 = compute_exploitability(
        trained_solver, num_samples=10, num_rollouts_per_infoset=5, seed=123
    )

    # Should be different (with very high probability)
    assert results1["exploitability_mbb"] != results2["exploitability_mbb"]


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_average_vs_current_strategy(trained_solver):
    """Using average strategy should give more stable results."""
    # Average strategy is correct for Nash approximation
    avg_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=True, seed=42
    )

    curr_results = compute_exploitability(
        trained_solver, num_samples=10, use_average_strategy=False, seed=42
    )

    # Both should be positive
    assert avg_results["exploitability_mbb"] > 0
    assert curr_results["exploitability_mbb"] > 0

    # Average strategy should generally be less exploitable
    # (but not strict due to sampling variance)


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_total_positive_regret_structure(trained_solver):
    """Test regret computation returns expected structure."""
    results = compute_total_positive_regret(trained_solver)

    required_fields = [
        "total_positive_regret",
        "num_infosets",
        "avg_regret_per_infoset",
    ]

    for field in required_fields:
        assert field in results, f"Missing field: {field}"

    # Sanity checks
    assert results["total_positive_regret"] >= 0
    assert results["num_infosets"] > 0
    assert results["avg_regret_per_infoset"] >= 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_total_positive_regret_decreases(simple_config):
    """Regret should decrease with more training."""
    action_abstraction = ActionAbstraction(big_blind=simple_config.get("game.big_blind", 100))
    card_abstraction = DummyCardAbstraction()
    storage = InMemoryStorage()

    solver = MCCFRSolver(
        action_abstraction,
        card_abstraction,
        storage,
        config={
            "starting_stack": simple_config.get("game.starting_stack", 10000),
            "small_blind": simple_config.get("game.small_blind", 50),
            "big_blind": simple_config.get("game.big_blind", 100),
            "seed": 42,
        },
    )

    # Initial regret (reduced iterations)
    for _ in range(10):
        solver.train_iteration()
    regret_early = compute_total_positive_regret(solver)

    # Train more
    for _ in range(20):
        solver.train_iteration()
    regret_late = compute_total_positive_regret(solver)

    # Regret should decrease (general trend)
    print(f"Early regret: {regret_early['total_positive_regret']:.2e}")
    print(f"Late regret: {regret_late['total_positive_regret']:.2e}")

    # At minimum, both should be non-negative
    assert regret_early["total_positive_regret"] >= 0
    assert regret_late["total_positive_regret"] >= 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_zero_samples_fails(trained_solver):
    """Zero samples should return NaN values."""
    results = compute_exploitability(trained_solver, num_samples=0)
    # With zero samples, should get NaN results
    assert np.isnan(results["exploitability_mbb"])


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_large_rollout_budget_works(trained_solver):
    """Should handle large rollout budgets without errors."""
    results = compute_exploitability(
        trained_solver, num_samples=5, num_rollouts_per_infoset=200, seed=42
    )

    assert results["exploitability_mbb"] > 0


@pytest.mark.slow
@pytest.mark.timeout(20)
def test_exploitability_units_conversion(trained_solver):
    """Test mbb and bb conversions are consistent."""
    results = compute_exploitability(trained_solver, num_samples=10, seed=42)

    # mbb should be 1000x bb
    mbb = results["exploitability_mbb"]
    bb = results["exploitability_bb"]

    np.testing.assert_allclose(mbb, bb * 1000, rtol=1e-5)
