"""Tests for exploitability computation."""

from types import SimpleNamespace
from typing import Any, cast

import numpy as np
import pytest

from src.actions.betting_actions import BettingActions
from src.evaluation import exploitability as exploitability_module
from src.evaluation.exploitability import compute_exploitability
from src.solver.mccfr import MCCFRSolver
from src.solver.storage.shared_array import SharedArrayStorage
from tests.test_helpers import DummyCardAbstraction, make_test_config


class TestExploitability:
    """Tests for exploitability computation."""

    def test_compute_exploitability_untrained_solver(self):
        """Test exploitability of an untrained solver (should be high)."""
        # Create solver
        action_abstraction = BettingActions()
        card_abstraction = DummyCardAbstraction()
        storage = SharedArrayStorage(
            num_workers=1, worker_id=0, session_id="test", is_coordinator=True
        )

        solver = MCCFRSolver(
            action_abstraction,
            card_abstraction,
            storage,
            config=make_test_config(starting_stack=200, small_blind=1, big_blind=2),
        )

        # Compute exploitability with very small sample size for speed (just 10 samples)
        result = compute_exploitability(solver, num_samples=10, use_average_strategy=True)

        # Check result structure
        assert "exploitability_mbb" in result
        assert "player_0_br_utility" in result
        assert "player_1_br_utility" in result

        # Exploitability should be a number
        assert isinstance(result["exploitability_mbb"], float)

        # For untrained solver, exploitability should be relatively high
        # (uniform random play is very exploitable)
        print(f"Untrained exploitability: {result['exploitability_mbb']:.2f} mbb/g")

    @pytest.mark.slow
    @pytest.mark.timeout(20)
    def test_compute_exploitability_trained_solver(self):
        """Test exploitability of a trained solver (should be lower)."""
        # Create solver
        action_abstraction = BettingActions()
        card_abstraction = DummyCardAbstraction()
        storage = SharedArrayStorage(
            num_workers=1, worker_id=0, session_id="test", is_coordinator=True
        )

        solver = MCCFRSolver(
            action_abstraction,
            card_abstraction,
            storage,
            config=make_test_config(starting_stack=200, small_blind=1, big_blind=2, seed=42),
        )

        for _ in range(5):
            solver.train_iteration()

        # Compute exploitability with multiple samples for stable estimate
        # With num_samples=1, variance is too high and can give negative results
        result = compute_exploitability(solver, num_samples=10, use_average_strategy=True, seed=123)

        exploitability = result["exploitability_mbb"]
        assert isinstance(exploitability, (float, int))
        print(f"Trained (5 iters) exploitability: {exploitability:.2f} mbb/g")

        # After training, exploitability should be non-negative
        # With Monte Carlo estimation, expect reasonable bounds
        assert exploitability >= 0
        assert exploitability < 50000  # Very loose bound for minimal training

    @pytest.mark.slow
    @pytest.mark.timeout(20)
    def test_compute_exploitability_different_strategies(self):
        """Test that average strategy and current strategy give different results."""
        # Create and train solver
        action_abstraction = BettingActions()
        card_abstraction = DummyCardAbstraction()
        storage = SharedArrayStorage(
            num_workers=1, worker_id=0, session_id="test", is_coordinator=True
        )

        solver = MCCFRSolver(
            action_abstraction,
            card_abstraction,
            storage,
            config=make_test_config(starting_stack=200, small_blind=1, big_blind=2, seed=42),
        )

        for _ in range(15):
            solver.train_iteration()

        # Compute both with minimal samples
        avg_result = compute_exploitability(solver, num_samples=1, use_average_strategy=True)
        current_result = compute_exploitability(solver, num_samples=1, use_average_strategy=False)

        avg_exploitability = avg_result["exploitability_mbb"]
        current_exploitability = current_result["exploitability_mbb"]
        assert isinstance(avg_exploitability, (float, int))
        assert isinstance(current_exploitability, (float, int))
        print(f"Average strategy exploitability: {avg_exploitability:.2f} mbb/g")
        print(f"Current strategy exploitability: {current_exploitability:.2f} mbb/g")

        # Both should be valid numbers
        assert avg_exploitability >= 0
        assert current_exploitability >= 0


class TestExploitabilityConfidenceIntervals:
    """Tests for statistical correctness of exploitability uncertainty."""

    def test_std_error_uses_paired_samples(self, monkeypatch):
        """
        Standard error should be computed from paired exploitability samples.

        Construct perfectly anti-correlated player BR utilities:
          u0_i = +(i + 1), u1_i = -(i + 1)
        Then paired exploitability samples are all zero:
          e_i = (u0_i + u1_i) / 2 = 0
        So SE must be exactly zero.
        """

        def fake_br(_solver, state, br_player, _use_average_strategy, _num_rollouts):
            val = float(state + 1)
            return val if br_player == 0 else -val

        monkeypatch.setattr(exploitability_module, "_compute_rollout_best_response", fake_br)

        next_state = 0

        def deal_state():
            nonlocal next_state
            state = next_state
            next_state += 1
            return state

        solver: Any = SimpleNamespace(big_blind=2, _deal_initial_state=deal_state)
        results = compute_exploitability(
            cast(MCCFRSolver, solver),
            num_samples=5,
            use_average_strategy=True,
            num_rollouts_per_infoset=1,
        )

        assert results["exploitability_bb"] == pytest.approx(0.0)
        assert results["std_error_mbb"] == pytest.approx(0.0, abs=1e-12)
        ci_lower, ci_upper = cast(tuple[float, float], results["confidence_95_mbb"])
        assert ci_lower == pytest.approx(0.0, abs=1e-12)
        assert ci_upper == pytest.approx(0.0, abs=1e-12)

    def test_std_error_matches_paired_sample_formula(self, monkeypatch):
        """
        Validate SE against the paired-sample estimator.

        Construct perfectly correlated utilities:
          u0_i = u1_i = i
        Then e_i = i and the expected SE is:
          std(e, ddof=1) / sqrt(n)
        """

        def fake_br(_solver, state, _br_player, _use_average_strategy, _num_rollouts):
            return float(state)

        monkeypatch.setattr(exploitability_module, "_compute_rollout_best_response", fake_br)

        next_state = 1

        def deal_state():
            nonlocal next_state
            state = next_state
            next_state += 1
            return state

        solver: Any = SimpleNamespace(big_blind=2, _deal_initial_state=deal_state)
        num_samples = 5
        results = compute_exploitability(
            cast(MCCFRSolver, solver),
            num_samples=num_samples,
            use_average_strategy=True,
            num_rollouts_per_infoset=1,
        )

        paired_samples = np.array([1, 2, 3, 4, 5], dtype=np.float64)
        expected_se_chips = np.std(paired_samples, ddof=1) / np.sqrt(num_samples)
        expected_se_mbb = (expected_se_chips / solver.big_blind) * 1000
        old_independent_assumption_mbb = expected_se_mbb / np.sqrt(2.0)

        assert results["std_error_mbb"] == pytest.approx(expected_se_mbb)
        # Regression check: old independence formula would underestimate SE here.
        assert results["std_error_mbb"] > old_independent_assumption_mbb * 1.3
